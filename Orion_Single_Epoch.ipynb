{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b4bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as backend \n",
    "import io\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import time \n",
    "import boto3\n",
    "import boto.s3\n",
    "import sys\n",
    "import json\n",
    "from orion import Orion\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "from mlblocks import MLPipeline\n",
    "from orion.evaluation import CONTEXTUAL_METRICS as METRICS\n",
    "from orion.evaluation import contextual_confusion_matrix\n",
    "from functools import partial\n",
    "from orion.data import load_signal, load_anomalies\n",
    "from sklearn.metrics import mean_squared_error\n",
    "S3_URL = 'https://d3-ai-orion-analysis.s3.amazonaws.com/'\n",
    "BUCKET = \"d3-ai-orion-analysis\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f64ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_name = 'S-1'\n",
    "# load signal\n",
    "train_data = load_signal(signal_name)\n",
    "# load ground truth anomalies\n",
    "# known_anomalies = load_anomalies(signal)\n",
    "\n",
    "df = pd.DataFrame(train_data[\"value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f0b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(data, start=0):\n",
    "    index = np.array(range(start, start + len(data)))\n",
    "    step = 300\n",
    "    initial_time = 1222819200 \n",
    "    timestamp = index * step + initial_time \n",
    "    return pd.DataFrame({'timestamp': timestamp}, dtype='int64')\n",
    "def time_index(index):\n",
    "    step = 300\n",
    "    initial_time = 1222819200\n",
    "    return index * step + initial_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab2023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creds():    \n",
    "    credentials_file = \"credentials_file.txt\" #some path\n",
    "    with open(credentials_file) as file:\n",
    "        lines = file.readlines()\n",
    "        ACCESS_KEY_ID_LINE_NUM = 2\n",
    "        SECRET_ACCESS_KEY_ID_LINE_NUM = 3\n",
    "        access_key_id = lines[ACCESS_KEY_ID_LINE_NUM]\n",
    "        secret_access_key = lines[SECRET_ACCESS_KEY_ID_LINE_NUM]\n",
    "        \n",
    "        access_key_id = access_key_id.replace(\"\\n\", \"\")\n",
    "        secret_access_key = secret_access_key.replace(\"\\n\", \"\")\n",
    "    return access_key_id, secret_access_key    \n",
    "    \n",
    "def send_file_to_s3(file_path, file_name): \n",
    "    # Creating Session With Boto3.\n",
    "    # Creating S3 Resource From the Session.\n",
    "    # Fetch S3 credentials from credentials file\n",
    "    print(\"Sending file {file_name} to S3\".format(file_name=file_name))\n",
    "    access_key_id, secret_access_key = get_creds()\n",
    "    s3 = boto3.Session(\n",
    "        aws_access_key_id = access_key_id,\n",
    "        aws_secret_access_key = secret_access_key\n",
    "    ).resource('s3')\n",
    "    #     'https://d3-ai-orion-analysis.s3.amazonaws.com/'\n",
    "    result = s3.Bucket(BUCKET).upload_file(file_path, file_name)\n",
    "\n",
    "def load_file_from_s3(file_name):\n",
    "    # Fetch S3 credentials from credentials file\n",
    "    access_key_id, secret_access_key = get_creds()\n",
    "    s3 = boto3.Session(\n",
    "        aws_access_key_id = access_key_id,\n",
    "        aws_secret_access_key = secret_access_key\n",
    "    ).resource('s3')\n",
    "    \n",
    "    file = s3.Object(BUCKET, file_name).get()['Body'].read()\n",
    "    \n",
    "    return file\n",
    "\n",
    "def delete_file_from_s3(file_name):\n",
    "    \n",
    "    access_key_id, secret_access_key = get_creds()\n",
    "    s3 = boto3.Session(\n",
    "        aws_access_key_id = access_key_id,\n",
    "        aws_secret_access_key = secret_access_key\n",
    "    ).resource('s3')\n",
    "        \n",
    "    return s3.Object(BUCKET, file_name).delete() # your-key is the signature of the object you want to delete\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c12ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_anomalies(signal_name):\n",
    "    anomalies = list()\n",
    "    \n",
    "    if signal_name == 'S-1':\n",
    "        anomalies = load_file_from_s3('anomalies.csv')\n",
    "\n",
    "    # Might need to implement other way of retrieving anomalies\n",
    "    return anomalies\n",
    "\n",
    "# get_ground_truth_anomalies('S-1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# del metrics['accuracy']\n",
    "def get_scores(orion, signal_name, original_time_series, gen_time_series, detected_anomalies):\n",
    "    \"\"\"\n",
    "    Extract Confusion Matrix, F1 Scores and MSE from data\n",
    "    \"\"\"\n",
    "    METRICS['confusion_matrix'] =  contextual_confusion_matrix\n",
    "    \n",
    "    metrics_overlap = METRICS.copy()\n",
    "    \n",
    "    metrics_overlap = {name + '_overlap': partial(fun, weighted=False) for name, fun in metrics_overlap.items()}\n",
    "\n",
    "    metrics = {**METRICS, **metrics_overlap}\n",
    "    # dict of {[name of fn] : fn itself}\n",
    "    \n",
    "    # scorer input\n",
    "    # ground truth anomalies (from anomalies.csv in s3bucket)\n",
    "    # detected anomalies (output of orion/pipeline)\n",
    "    # data (signal)\n",
    "    train_data = load_signal(signal_name)\n",
    "    ground_truth_anomalies = get_ground_truth_anomalies(signal_name)\n",
    "    data = train_data\n",
    "    scores = {}\n",
    "    for name, scorer in metrics.items():\n",
    "        try:\n",
    "            scores[name] = scorer(ground_truth_anomalies, detected_anomalies, data)\n",
    "        except Exception as ex:\n",
    "            scores[name] = 0\n",
    "    data = pd.DataFrame(train_data[\"value\"])\n",
    "    \n",
    "    mse = mean_squared_error(original_time_series, gen_time_series)\n",
    "    scores[\"MSE\"] = mse\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405cc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff76af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_name_for_signal(signal_name):\n",
    "    \n",
    "    dataset_file_name = \"datasets_names.csv \"\n",
    "    datasets_to_signals = dict()\n",
    "    # This program assumes the csv file has a header. \n",
    "    # In case of missing header in the csv file, we have to pass it explicitly to the program\n",
    "    csv_file = pd.DataFrame(pd.read_csv(dataset_file_name, sep = \",\", header = None, index_col = False))\n",
    "    csv_file.to_json(\"datasets.json\", orient = \"records\", date_format = \"epoch\", double_precision = 10, force_ascii = True, date_unit = \"ms\", default_handler = None)\n",
    "    \n",
    "    with open(\"datasets.json\") as f:\n",
    "        json_list = json.load(f)\n",
    "        \n",
    "    \n",
    "    for json_dict in json_list:\n",
    "        dataset_name = json_dict['0'] \n",
    "        corresponding_signals = json_dict['1']\n",
    "        \n",
    "        datasets_to_signals[dataset_name] = corresponding_signals\n",
    "    \n",
    "    signals_to_datasets = dict()\n",
    "    \n",
    "    for dataset_name in datasets_to_signals.keys():\n",
    "        formatted_signals_list = datasets_to_signals[dataset_name].replace('\\'', '').replace('(', '').replace(')', '').replace(' ', '').split(',')\n",
    "        for signal in formatted_signals_list:\n",
    "            \n",
    "            signals_to_datasets[signal] = dataset_name\n",
    "    \n",
    "    return signals_to_datasets[signal_name]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb23f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_anomalies_timeseries_scores(orion, signal_name, current_epoch, pkl_file_substring):\n",
    "    train_data = load_signal(signal_name)\n",
    "    anomalies, viz = orion.detect(train_data, visualization=True)\n",
    "    \n",
    "    gen_time_series = viz[\"generated_timeseries\"]\n",
    "    original_time_series = viz[\"original_timeseries\"]\n",
    "    \n",
    "    # SAVE F1 Scores and MSEs using VIZ \n",
    "    # GET F1 SCORES using get_scores()\n",
    "    scores = get_scores(orion, signal_name, original_time_series, gen_time_series, anomalies)\n",
    "\n",
    "    # Creates local file path to load from\n",
    "    anomalies_path = os.path.join(os.path.abspath('.'), pkl_file_substring + \"epoch_{current_epoch}-anomalies.pkl\".format(current_epoch=current_epoch))\n",
    "    gen_time_series_path = os.path.join(os.path.abspath('.'), pkl_file_substring + \"epoch_{current_epoch}-generated_timeseries.pkl\".format(current_epoch=current_epoch))\n",
    "    orion_path = os.path.join(os.path.abspath('.'), pkl_file_substring + \"epoch_{current_epoch}-orion.pkl\".format(current_epoch=current_epoch))\n",
    "    report_path = os.path.join(os.path.abspath('.'), pkl_file_substring + \"epoch_{current_epoch}-report.pkl\".format(current_epoch=current_epoch))\n",
    "        \n",
    "    # UPDATE EPOCH NUMBER PER NAMING \n",
    "    anomalies_file_name = pkl_file_substring + \"/epoch_{current_epoch}_anomalies.pkl\".format(current_epoch=current_epoch)\n",
    "    gen_time_series_file_name = pkl_file_substring + \"/epoch_{current_epoch}_generated_timeseries.pkl\".format(current_epoch=current_epoch)\n",
    "    orion_file_name = pkl_file_substring + \"/epoch_{current_epoch}_orion.pkl\".format(current_epoch=current_epoch)\n",
    "    report_file_name = pkl_file_substring + \"/epoch_{current_epoch}_report.pkl\".format(current_epoch=current_epoch)\n",
    "    \n",
    "    with open(anomalies_path, 'wb') as new_file:\n",
    "        pickle.dump(anomalies, new_file)\n",
    "        # Send Anomalies to S3\n",
    "        send_file_to_s3(anomalies_path, anomalies_file_name)\n",
    "    with open(gen_time_series_path, 'wb') as new_file:\n",
    "        pickle.dump(gen_time_series, new_file)\n",
    "        # Send generated time series to S3\n",
    "        send_file_to_s3(gen_time_series_path, gen_time_series_file_name)\n",
    "    \n",
    "    with open(orion_path, 'wb') as new_file:\n",
    "        pickle.dump(orion, new_file)\n",
    "        # Send ORION to S3\n",
    "        send_file_to_s3(orion_path, orion_file_name)\n",
    "    \n",
    "    with open(report_path, 'wb') as new_file:\n",
    "        pickle.dump(scores, new_file)\n",
    "        # Sedn SCORE REPORT TO S3\n",
    "        send_file_to_s3(report_path, report_file_name)\n",
    "    \n",
    "    return anomalies, gen_time_series, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec7b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# Num Epochs\n",
    "# Strat\n",
    "def get_date_index():\n",
    "    step = 300\n",
    "    initial_time = 1222819200\n",
    "    \n",
    "    time_struct = time.localtime()\n",
    "    month, day = time_struct.tm_mon, time_struct.tm_mday\n",
    "    hour, minute, sec = time_struct.tm_hour, time_struct.tm_min, time_struct.tm_sec\n",
    "        \n",
    "    ## CONVERT TO INTEGER WITH TIME STAMP\n",
    "    index = month * 30 * 3600 * 24 + day * 3600 * 24 + hour * 3600 + minute * 60 + sec \n",
    "    index = index % step\n",
    "    date_index =  str(index * step + initial_time)\n",
    "    return date_index\n",
    "\n",
    "def create_pipeline_file_name(signal_name, pipeline_name, num_epochs, date_index):\n",
    "    ### FOLDER NAME_PIPELINE NAME_SIGNAL NAME_DATASET NAME_NUMEPOCHS, DATESTAMP,  . pkl \n",
    "    dataset_name = get_dataset_name_for_signal(signal_name)\n",
    "    pkl_file_name = \"{pipeline_name}_num_epochs_{num_epochs}_{signal_name}_{dataset_name}_{date_index}\".format(pipeline_name=pipeline_name, num_epochs=num_epochs, \n",
    "                    signal_name=signal_name, dataset_name=dataset_name,date_index=date_index)\n",
    "    pkl_file_name += \".pkl\"\n",
    "    \n",
    "    return pkl_file_name\n",
    "\n",
    "def single_epoch_pipeline(num_epochs, pipeline_name, signal_name, s3_path = None):\n",
    "    \n",
    "    # WARNING: OS DEPENDENT- WINDOWS = \\\\, LINUX = /\n",
    "    ##################################################\n",
    "    train_data = load_signal(signal_name)\n",
    "    # FETCH TIME and then use it to create pipeline name \n",
    "    date_index = get_date_index()\n",
    "    pkl_file_name = create_pipeline_file_name(signal_name, pipeline_name, num_epochs, date_index)\n",
    "    new_path = os.path.join(os.path.abspath('.'), pkl_file_name)\n",
    "    \n",
    "    print(new_path)\n",
    "    \n",
    "    if s3_path:\n",
    "        # Load pkl file content from S3\n",
    "        pkl_file = load_file_from_s3(s3_path)\n",
    "        # Creates local file path to load from\n",
    "        with open(new_path, 'wb') as new_file:\n",
    "            pickle.dump(pkl_file, new_file)   \n",
    "        \n",
    "        orion.load(new_path)\n",
    "        \n",
    "    elif s3_path is None:\n",
    "        # initialize pipeline if no previous pipeline data\n",
    "        hyperparameters = {\n",
    "            'keras.Sequential.LSTMTimeSeriesRegressor#1': {\n",
    "                'epochs': 1,\n",
    "                'verbose': True\n",
    "            }  \n",
    "        }\n",
    "        lstm_viz_path = \"lstm_dynamic_threshold_viz\"\n",
    "        orion = Orion(lstm_viz_path, hyperparameters)\n",
    "        # Creates local file path to load from\n",
    "        # First time \n",
    "        orion.fit(train_data)\n",
    "        current_epoch = 0\n",
    "        anomalies, gen_time_series, scores = send_anomalies_timeseries_scores(orion, signal_name, current_epoch, pkl_file_name[:-4])\n",
    "        \n",
    "        orion.save(new_path)\n",
    "    # Caching and then deleting \n",
    "    num_iterations = num_epochs if s3_path else num_epochs - 1\n",
    "    for i in range(num_iterations):\n",
    "        current_epoch += 1\n",
    "        backend.clear_session()\n",
    "        orion = Orion.load(new_path)\n",
    "        orion.fit(train_data)\n",
    "        \n",
    "        ## Extract MSE, F1 scores from here\n",
    "        ## Check weights are actually changing from epoch to epoch\n",
    "        print(\"LOOP TIME!!!\")\n",
    "        anomalies, gen_time_series, scores = send_anomalies_timeseries_scores(orion, signal_name, current_epoch, pkl_file_name[:-4])\n",
    "        orion.save(new_path)\n",
    "    \n",
    "    return orion\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e4376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rarh9\\Desktop\\MIT\\UROPs\\Orion\\orion_venv\\MSE_F1\\lstm_dynamic_threshold_num_epochs_2_S-1_SMAP_1222906200.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rarh9\\Desktop\\MIT\\UROPs\\Orion\\orion_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\rarh9\\Desktop\\MIT\\UROPs\\Orion\\orion_venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 7919 samples, validate on 1980 samples\n",
      "Epoch 1/1\n",
      "7919/7919 [==============================] - 113s 14ms/step - loss: 0.1947 - mse: 0.1947 - val_loss: 0.3870 - val_mse: 0.3870\n",
      "Weights and biases of the layers after training the model: \n",
      "\n",
      "lstm_1\n",
      "Weights\n",
      "[array([[-0.02070745, -0.08736204, -0.01364249,  0.02164745, -0.03996319,\n",
      "        -0.06233378, -0.11881439, -0.02196616,  0.05102506,  0.02173549,\n",
      "         0.07576918,  0.03108119, -0.07425395, -0.01828902,  0.06020026,\n",
      "         0.08213782,  0.09327428,  0.08369874, -0.03157612,  0.14428584,\n",
      "         0.05088127,  0.0015283 , -0.06786935, -0.07828359,  0.10508209,\n",
      "         0.00971093,  0.05923137,  0.11840479,  0.15169008,  0.14253458,\n",
      "         0.11099122, -0.07932208,  0.10861377, -0.00557674, -0.01183332,\n",
      "         0.05544175, -0.00657045,  0.12735915,  0.14856862,  0.00242854,\n",
      "        -0.01771243, -0.06051758, -0.12180395, -0.10477857,  0.09844823,\n",
      "         0.01677408,  0.01561569, -0.07298386,  0.00926842, -0.11733459,\n",
      "         0.11515676, -0.03750393, -0.06640875, -0.08526737, -0.09627974,\n",
      "         0.08541875,  0.05780753,  0.04540349, -0.00750317,  0.02471907,\n",
      "        -0.0561788 ,  0.10195436, -0.12298867,  0.13455638,  0.09145526,\n",
      "        -0.01797304, -0.07250933, -0.01145892, -0.08805969,  0.11670763,\n",
      "         0.06769224, -0.06589359,  0.05580154,  0.00703595,  0.11078547,\n",
      "        -0.09633157, -0.01201901, -0.00760353, -0.11878929,  0.064164  ,\n",
      "         0.03858228, -0.02100127, -0.05000512,  0.05822537,  0.01950657,\n",
      "        -0.01338074,  0.07273744, -0.0090167 , -0.02857499,  0.02749515,\n",
      "        -0.02021731, -0.03207955, -0.07818632, -0.01086927, -0.0078083 ,\n",
      "         0.08749299, -0.08560565, -0.11323424,  0.07585485, -0.1431361 ,\n",
      "         0.00223801, -0.08054934, -0.06289239, -0.05004422, -0.1323117 ,\n",
      "        -0.06230471, -0.12155952,  0.10511408, -0.08139154, -0.06983971,\n",
      "        -0.02718716,  0.11571971,  0.10557792,  0.0755326 , -0.12947786,\n",
      "         0.10229976,  0.07137231, -0.1400949 ,  0.08613756,  0.09474622,\n",
      "        -0.10844326,  0.06652031, -0.07088469, -0.00929823,  0.09208912,\n",
      "        -0.04860337, -0.03944941, -0.11555506,  0.06898024,  0.10591435,\n",
      "        -0.05898701,  0.12334293, -0.0586157 , -0.09683287, -0.09033147,\n",
      "        -0.10688089, -0.00137238, -0.0503501 ,  0.03391979,  0.08444592,\n",
      "         0.1128021 , -0.04139557,  0.00038102, -0.00662033, -0.09616711,\n",
      "        -0.12983312, -0.04074092, -0.12993751, -0.12620732, -0.05692972,\n",
      "         0.04208761,  0.01597885, -0.07801884, -0.03005584, -0.0540125 ,\n",
      "         0.05001427,  0.09324878,  0.12255959, -0.00340848,  0.02542705,\n",
      "         0.08482017, -0.03007133,  0.03675029, -0.04684957, -0.03814987,\n",
      "        -0.06211468,  0.01823089, -0.01464046,  0.03445674, -0.1041794 ,\n",
      "         0.03353211,  0.03360303,  0.10025954, -0.10908228,  0.04231101,\n",
      "        -0.13729094, -0.10633866,  0.00585179, -0.08912062, -0.1182887 ,\n",
      "        -0.04636353,  0.08493424, -0.02580081,  0.06239814,  0.03553425,\n",
      "         0.1129261 , -0.13492003, -0.11815121, -0.12803885,  0.10501461,\n",
      "        -0.07004713, -0.03128118,  0.08977939, -0.01923745,  0.06007908,\n",
      "         0.04750244, -0.08451254,  0.01561576, -0.12293682,  0.05893103,\n",
      "         0.06288775, -0.02829126,  0.06622206, -0.00482127,  0.07880638,\n",
      "         0.066668  , -0.08865264, -0.06046928, -0.0081822 , -0.05755749,\n",
      "        -0.10969482,  0.13518597,  0.03988095,  0.03192206, -0.02207357,\n",
      "         0.03510765, -0.06917793,  0.03794699,  0.04754605, -0.08056645,\n",
      "        -0.10443179, -0.06445897, -0.08290053, -0.13048652,  0.00056503,\n",
      "        -0.11840417,  0.12885994, -0.07295927, -0.07163301,  0.0750794 ,\n",
      "        -0.01706873, -0.02481563, -0.02143433, -0.01660395,  0.02488102,\n",
      "        -0.0061594 , -0.11834375,  0.00409502,  0.07130452,  0.06041891,\n",
      "        -0.09887531,  0.1607872 ,  0.02326623, -0.06442931, -0.07029937,\n",
      "         0.11194873, -0.03931481, -0.10645653, -0.01233569, -0.01859323,\n",
      "        -0.06169615, -0.11668854,  0.1612691 ,  0.08133836,  0.01223674,\n",
      "         0.12820214, -0.05733984,  0.02992564, -0.12665756, -0.00262984,\n",
      "        -0.02516931,  0.14201038,  0.1531799 ,  0.096825  , -0.07808645,\n",
      "        -0.05990781,  0.00796907,  0.02335434, -0.1004917 ,  0.07164062,\n",
      "         0.11478608,  0.12626502, -0.10898611, -0.07255966,  0.0674318 ,\n",
      "        -0.00498418, -0.05038242, -0.10610247, -0.02133498, -0.04176035,\n",
      "         0.10619928,  0.05661001, -0.07654949,  0.01394007,  0.08330956,\n",
      "         0.06035656, -0.08479305,  0.15443826,  0.08229451, -0.09283932,\n",
      "        -0.07914419, -0.11423322, -0.05783118,  0.04079096, -0.11598859,\n",
      "         0.02077371,  0.10962705, -0.00314386,  0.00963885, -0.0732843 ,\n",
      "        -0.04897279, -0.08651503, -0.11474387, -0.09715403,  0.07025671,\n",
      "        -0.0991167 , -0.04733786,  0.03783112,  0.11056332, -0.0334691 ,\n",
      "        -0.00208943,  0.15992293, -0.00155111,  0.15985875,  0.026864  ,\n",
      "         0.02908101,  0.02936521, -0.09514509, -0.08303554, -0.02379262]],\n",
      "      dtype=float32), array([[-0.11311219,  0.06576069,  0.03763254, ...,  0.06111562,\n",
      "         0.05871271,  0.06285384],\n",
      "       [ 0.02818039, -0.10108345, -0.07411882, ...,  0.06221562,\n",
      "        -0.0606363 , -0.09452772],\n",
      "       [-0.04560523,  0.00243175,  0.07159352, ...,  0.09895619,\n",
      "         0.01981669,  0.02738908],\n",
      "       ...,\n",
      "       [ 0.03292413,  0.07185254,  0.03115176, ...,  0.01568024,\n",
      "        -0.0578251 , -0.08914649],\n",
      "       [ 0.01428226,  0.03302304,  0.04505054, ...,  0.00675484,\n",
      "         0.01761797,  0.06017891],\n",
      "       [ 0.0693232 ,  0.12381289,  0.03126244, ..., -0.02335318,\n",
      "         0.13489054,  0.08691631]], dtype=float32), array([-2.41875160e-03,  3.06308339e-03, -1.94958728e-02, -1.48560219e-02,\n",
      "        4.90867347e-03, -8.86047073e-03, -9.15759895e-03, -1.16771609e-02,\n",
      "       -6.31912006e-03, -9.09603760e-03, -5.73423970e-03,  2.75775557e-04,\n",
      "        8.77614971e-03, -1.37831597e-02,  3.59195488e-04,  8.69458541e-03,\n",
      "       -6.78315468e-04, -7.10034324e-03,  8.27777293e-03,  3.00720567e-03,\n",
      "       -1.83090474e-02, -8.00206326e-03,  2.35337368e-03, -2.03405637e-02,\n",
      "        9.73720383e-03,  2.69551435e-03,  5.02327830e-03,  2.83852278e-05,\n",
      "        1.57994055e-03,  2.05824268e-03,  5.42667694e-04,  1.29583373e-03,\n",
      "       -2.47411728e-02, -2.52326131e-02,  1.09554091e-02, -8.53012409e-03,\n",
      "       -9.62512661e-03,  3.26544116e-03,  6.70658611e-03,  3.44056287e-04,\n",
      "       -1.92081183e-02, -9.99195501e-03, -7.65387807e-03,  5.81334718e-03,\n",
      "       -3.57752177e-03, -1.06246192e-02,  3.77825554e-03,  1.33019593e-02,\n",
      "        2.96324003e-03, -8.69669765e-03,  1.01710046e-02, -2.36226153e-03,\n",
      "       -1.62770897e-02,  9.13024135e-03,  3.03469761e-03,  7.85750616e-03,\n",
      "       -9.58066806e-03, -7.67860282e-03, -6.34699967e-03, -1.28538851e-02,\n",
      "        1.61154219e-03, -1.91582192e-03, -1.11106830e-03,  6.85719494e-03,\n",
      "       -5.06645953e-03, -1.90438994e-03,  5.48221171e-03, -5.27714333e-03,\n",
      "        8.46877415e-03, -1.84387397e-02, -7.38299871e-03, -1.12154605e-02,\n",
      "       -2.11113179e-03, -4.13163984e-03,  1.46811893e-02, -4.83600609e-03,\n",
      "       -6.70270901e-03, -4.15632920e-03, -6.55833539e-03, -6.08331850e-03,\n",
      "        9.91790473e-01,  1.00281715e+00,  9.79869783e-01,  1.00409925e+00,\n",
      "        1.01295733e+00,  9.91341114e-01,  9.91332412e-01,  9.93167758e-01,\n",
      "        9.96592045e-01,  9.98243451e-01,  1.00366533e+00,  1.01479542e+00,\n",
      "        1.00251353e+00,  9.99363780e-01,  1.01085150e+00,  1.01341057e+00,\n",
      "        9.92295265e-01,  1.00322354e+00,  1.00125301e+00,  9.96222317e-01,\n",
      "        9.91989553e-01,  9.86075819e-01,  1.01346719e+00,  9.81615722e-01,\n",
      "        1.01557946e+00,  9.94768798e-01,  1.00949264e+00,  1.01145458e+00,\n",
      "        1.01524830e+00,  1.00841439e+00,  1.01259553e+00,  9.95098829e-01,\n",
      "        9.79342222e-01,  9.63722467e-01,  1.01697469e+00,  9.91310894e-01,\n",
      "        9.87047136e-01,  1.01467884e+00,  1.01189125e+00,  1.00749540e+00,\n",
      "        9.82110739e-01,  9.92452741e-01,  9.95850027e-01,  1.00810993e+00,\n",
      "        9.92901146e-01,  9.89370465e-01,  9.94445145e-01,  1.02175558e+00,\n",
      "        1.00250673e+00,  1.00124609e+00,  1.00614440e+00,  1.00014663e+00,\n",
      "        9.93085980e-01,  1.00283456e+00,  1.00368571e+00,  1.01280212e+00,\n",
      "        9.88479376e-01,  1.01480532e+00,  9.86632645e-01,  9.94812071e-01,\n",
      "        9.89902437e-01,  1.00624144e+00,  9.89607811e-01,  1.01190448e+00,\n",
      "        1.00111294e+00,  9.92377043e-01,  1.00139201e+00,  9.92606461e-01,\n",
      "        9.98067439e-01,  1.00318730e+00,  9.91949618e-01,  9.89985824e-01,\n",
      "        1.00272977e+00,  9.98700500e-01,  1.01185107e+00,  1.00156307e+00,\n",
      "        9.90302742e-01,  1.00314057e+00,  1.00458241e+00,  9.89991546e-01,\n",
      "       -6.12596655e-03,  6.88526686e-03,  5.18450281e-03, -1.00546358e-02,\n",
      "       -8.26921593e-03, -1.93625409e-03, -1.88388093e-03, -3.19771259e-03,\n",
      "        4.56388667e-03, -8.74356460e-03,  2.73957057e-03,  8.13219789e-03,\n",
      "        9.34247579e-03, -6.04454800e-03,  4.15486097e-03, -6.64480682e-03,\n",
      "        5.94990095e-03,  8.58237967e-03, -6.68334914e-03, -9.43942182e-03,\n",
      "       -3.88426660e-03,  2.61295703e-03, -6.33301772e-03, -5.22752805e-03,\n",
      "        6.80113258e-03, -4.25033679e-04, -1.33109251e-02, -2.74005486e-03,\n",
      "       -6.35326235e-03,  1.24626243e-02, -9.03008878e-03, -1.24006271e-02,\n",
      "        3.49664479e-03, -3.49983177e-03,  4.29028226e-03,  9.26578883e-03,\n",
      "       -4.26850421e-03, -1.66633390e-02, -8.37894343e-03,  4.82345140e-03,\n",
      "       -5.08041913e-03, -5.88019798e-03,  3.09534953e-03, -5.44899795e-03,\n",
      "       -4.08795755e-03,  2.55926372e-03,  3.46014020e-03, -1.04579926e-02,\n",
      "        5.45362756e-03, -8.44542496e-03, -4.89168614e-03, -2.67337193e-04,\n",
      "        1.11571024e-03, -9.43392888e-03, -8.37982167e-03, -9.79416817e-03,\n",
      "       -3.13420338e-03,  7.28268642e-03, -5.37875923e-04, -1.07913399e-02,\n",
      "        9.52527262e-05, -8.82222876e-03,  1.39855035e-03, -1.06997928e-02,\n",
      "       -3.57689243e-03,  6.77499804e-04, -6.15052460e-03, -6.30695466e-03,\n",
      "       -2.25513079e-03,  6.60559116e-03, -1.63178542e-03, -3.45601002e-04,\n",
      "       -3.61426990e-03, -6.97784871e-03, -6.96700951e-03,  4.23802948e-03,\n",
      "        1.74507219e-03, -4.70454665e-03,  1.08584575e-02,  5.85518451e-03,\n",
      "       -2.64407834e-03,  3.01111676e-03, -1.94736719e-02, -1.47864968e-02,\n",
      "        4.14702762e-03, -8.55643302e-03, -8.59340373e-03, -1.16609177e-02,\n",
      "       -6.04241528e-03, -9.24044754e-03, -4.87636682e-03,  4.07691055e-04,\n",
      "        5.03573706e-03, -1.44743267e-02,  7.90580874e-04,  1.01750325e-02,\n",
      "       -1.06779882e-03, -7.02965073e-03,  8.14844016e-03,  5.65191871e-03,\n",
      "       -1.77980997e-02, -9.12762154e-03,  1.45985198e-03, -2.02126913e-02,\n",
      "        1.04708755e-02,  2.86393706e-03,  5.97248739e-03,  1.63504761e-03,\n",
      "        6.63049472e-03,  2.99815508e-03,  2.85687228e-03, -3.01420921e-04,\n",
      "       -2.59641353e-02, -2.51827259e-02,  1.02644721e-02, -8.48023500e-03,\n",
      "       -9.67947952e-03,  3.79433529e-03,  8.98896437e-03,  6.29768590e-04,\n",
      "       -1.91219971e-02, -1.00319525e-02, -7.72839691e-03,  6.06887927e-03,\n",
      "       -3.79695999e-03, -1.06858769e-02,  4.01121425e-03,  1.11618442e-02,\n",
      "        2.63704779e-03, -8.28707218e-03,  1.14293089e-02, -2.42563803e-03,\n",
      "       -1.56638641e-02,  9.35190730e-03,  2.51393509e-03,  8.09402764e-03,\n",
      "       -9.55731887e-03, -6.99391309e-03, -6.44359738e-03, -1.21680144e-02,\n",
      "        1.09743536e-03, -9.85207735e-04, -1.59942079e-03,  9.97188874e-03,\n",
      "       -5.11883851e-03, -2.17540143e-03,  4.86087985e-03, -5.42422896e-03,\n",
      "        7.05047930e-03, -1.89299006e-02, -7.27401441e-03, -1.20125450e-02,\n",
      "       -1.93247793e-03, -4.56216373e-03,  1.42085347e-02, -4.96610627e-03,\n",
      "       -6.62226928e-03, -4.03896812e-03, -6.82623265e-03, -5.21079497e-03],\n",
      "      dtype=float32)]\n",
      "Shape:  (1, 320) \n",
      " [[-0.02070745 -0.08736204 -0.01364249  0.02164745 -0.03996319 -0.06233378\n",
      "  -0.11881439 -0.02196616  0.05102506  0.02173549  0.07576918  0.03108119\n",
      "  -0.07425395 -0.01828902  0.06020026  0.08213782  0.09327428  0.08369874\n",
      "  -0.03157612  0.14428584  0.05088127  0.0015283  -0.06786935 -0.07828359\n",
      "   0.10508209  0.00971093  0.05923137  0.11840479  0.15169008  0.14253458\n",
      "   0.11099122 -0.07932208  0.10861377 -0.00557674 -0.01183332  0.05544175\n",
      "  -0.00657045  0.12735915  0.14856862  0.00242854 -0.01771243 -0.06051758\n",
      "  -0.12180395 -0.10477857  0.09844823  0.01677408  0.01561569 -0.07298386\n",
      "   0.00926842 -0.11733459  0.11515676 -0.03750393 -0.06640875 -0.08526737\n",
      "  -0.09627974  0.08541875  0.05780753  0.04540349 -0.00750317  0.02471907\n",
      "  -0.0561788   0.10195436 -0.12298867  0.13455638  0.09145526 -0.01797304\n",
      "  -0.07250933 -0.01145892 -0.08805969  0.11670763  0.06769224 -0.06589359\n",
      "   0.05580154  0.00703595  0.11078547 -0.09633157 -0.01201901 -0.00760353\n",
      "  -0.11878929  0.064164    0.03858228 -0.02100127 -0.05000512  0.05822537\n",
      "   0.01950657 -0.01338074  0.07273744 -0.0090167  -0.02857499  0.02749515\n",
      "  -0.02021731 -0.03207955 -0.07818632 -0.01086927 -0.0078083   0.08749299\n",
      "  -0.08560565 -0.11323424  0.07585485 -0.1431361   0.00223801 -0.08054934\n",
      "  -0.06289239 -0.05004422 -0.1323117  -0.06230471 -0.12155952  0.10511408\n",
      "  -0.08139154 -0.06983971 -0.02718716  0.11571971  0.10557792  0.0755326\n",
      "  -0.12947786  0.10229976  0.07137231 -0.1400949   0.08613756  0.09474622\n",
      "  -0.10844326  0.06652031 -0.07088469 -0.00929823  0.09208912 -0.04860337\n",
      "  -0.03944941 -0.11555506  0.06898024  0.10591435 -0.05898701  0.12334293\n",
      "  -0.0586157  -0.09683287 -0.09033147 -0.10688089 -0.00137238 -0.0503501\n",
      "   0.03391979  0.08444592  0.1128021  -0.04139557  0.00038102 -0.00662033\n",
      "  -0.09616711 -0.12983312 -0.04074092 -0.12993751 -0.12620732 -0.05692972\n",
      "   0.04208761  0.01597885 -0.07801884 -0.03005584 -0.0540125   0.05001427\n",
      "   0.09324878  0.12255959 -0.00340848  0.02542705  0.08482017 -0.03007133\n",
      "   0.03675029 -0.04684957 -0.03814987 -0.06211468  0.01823089 -0.01464046\n",
      "   0.03445674 -0.1041794   0.03353211  0.03360303  0.10025954 -0.10908228\n",
      "   0.04231101 -0.13729094 -0.10633866  0.00585179 -0.08912062 -0.1182887\n",
      "  -0.04636353  0.08493424 -0.02580081  0.06239814  0.03553425  0.1129261\n",
      "  -0.13492003 -0.11815121 -0.12803885  0.10501461 -0.07004713 -0.03128118\n",
      "   0.08977939 -0.01923745  0.06007908  0.04750244 -0.08451254  0.01561576\n",
      "  -0.12293682  0.05893103  0.06288775 -0.02829126  0.06622206 -0.00482127\n",
      "   0.07880638  0.066668   -0.08865264 -0.06046928 -0.0081822  -0.05755749\n",
      "  -0.10969482  0.13518597  0.03988095  0.03192206 -0.02207357  0.03510765\n",
      "  -0.06917793  0.03794699  0.04754605 -0.08056645 -0.10443179 -0.06445897\n",
      "  -0.08290053 -0.13048652  0.00056503 -0.11840417  0.12885994 -0.07295927\n",
      "  -0.07163301  0.0750794  -0.01706873 -0.02481563 -0.02143433 -0.01660395\n",
      "   0.02488102 -0.0061594  -0.11834375  0.00409502  0.07130452  0.06041891\n",
      "  -0.09887531  0.1607872   0.02326623 -0.06442931 -0.07029937  0.11194873\n",
      "  -0.03931481 -0.10645653 -0.01233569 -0.01859323 -0.06169615 -0.11668854\n",
      "   0.1612691   0.08133836  0.01223674  0.12820214 -0.05733984  0.02992564\n",
      "  -0.12665756 -0.00262984 -0.02516931  0.14201038  0.1531799   0.096825\n",
      "  -0.07808645 -0.05990781  0.00796907  0.02335434 -0.1004917   0.07164062\n",
      "   0.11478608  0.12626502 -0.10898611 -0.07255966  0.0674318  -0.00498418\n",
      "  -0.05038242 -0.10610247 -0.02133498 -0.04176035  0.10619928  0.05661001\n",
      "  -0.07654949  0.01394007  0.08330956  0.06035656 -0.08479305  0.15443826\n",
      "   0.08229451 -0.09283932 -0.07914419 -0.11423322 -0.05783118  0.04079096\n",
      "  -0.11598859  0.02077371  0.10962705 -0.00314386  0.00963885 -0.0732843\n",
      "  -0.04897279 -0.08651503 -0.11474387 -0.09715403  0.07025671 -0.0991167\n",
      "  -0.04733786  0.03783112  0.11056332 -0.0334691  -0.00208943  0.15992293\n",
      "  -0.00155111  0.15985875  0.026864    0.02908101  0.02936521 -0.09514509\n",
      "  -0.08303554 -0.02379262]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5504/9899 [===============>..............] - ETA: 8s"
     ]
    }
   ],
   "source": [
    "pipeline_name = 'lstm_dynamic_threshold'\n",
    "signal_name = 'S-1'\n",
    "num_epochs = 2\n",
    "orion = single_epoch_pipeline(num_epochs, pipeline_name, signal_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eebd482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'EGJW3TPHQC985E4J',\n",
       "  'HostId': 'xATa8kLYPH83J7gvcGKZBitUH7bcjI/CEKL0QAdtzPV112yusdKSeAROEYQYS4JAkaM9mhDHyUw=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'xATa8kLYPH83J7gvcGKZBitUH7bcjI/CEKL0QAdtzPV112yusdKSeAROEYQYS4JAkaM9mhDHyUw=',\n",
       "   'x-amz-request-id': 'EGJW3TPHQC985E4J',\n",
       "   'date': 'Wed, 06 Apr 2022 18:20:10 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_file_from_s3('lstm_dynamic_threshold_num_epochs_1_S-1_SMAP_1222908000/anomalies.pkl')\n",
    "# delete_file_from_s3('lstm_dynamic_threshold_num_epochs_1_S-1_SMAP_1222908000/generated_timeseries.pkl')\n",
    "# delete_file_from_s3('lstm_dynamic_threshold_num_epochs_1_S-1_SMAP_1222908000/report.pkl')\n",
    "# delete_file_from_s3('lstm_dynamic_threshold_num_epochs_1_S-1_SMAP_1222908000/orion.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df3ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_file_to_s3(\"example.py\", \"ExampleFile.py\")\n",
    "\n",
    "# example_file = load_file_from_s3(\"ExampleFile.py\")\n",
    "# print(example_file)\n",
    "\n",
    "# example_file_csv = load_file_from_s3(\"anomalies.csv\")\n",
    "# print(example_file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13401b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an S3 client\n",
    "# s3_client = boto3.client('s3')\n",
    "\n",
    "# # Call to S3 to retrieve the policy for the given bucket\n",
    "# result = s3_client.get_bucket_acl(Bucket='d3-ai-orion-analysis')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073562d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"\"\"Orion Core module.\n",
    "\n",
    "# This module defines the Orion Class, which is responsible for the\n",
    "# main anomaly detection functionality, as well as the interaction\n",
    "# with the underlying MLBlocks pipelines.\n",
    "# \"\"\"\n",
    "# import json\n",
    "# import logging\n",
    "# import os\n",
    "# import pickle\n",
    "# from typing import List, Union\n",
    "\n",
    "# import pandas as pd\n",
    "# from mlblocks import MLPipeline\n",
    "\n",
    "# from orion.evaluation import CONTEXTUAL_METRICS as METRICS\n",
    "\n",
    "# LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# class Orion:\n",
    "#     \"\"\"Orion Class.\n",
    "\n",
    "#     The Orion Class provides the main anomaly detection functionalities\n",
    "#     of Orion and is responsible for the interaction with the underlying\n",
    "#     MLBlocks pipelines.\n",
    "\n",
    "#     Args:\n",
    "#         pipeline (str, dict or MLPipeline):\n",
    "#             Pipeline to use. It can be passed as:\n",
    "#                 * An ``str`` with a path to a JSON file.\n",
    "#                 * An ``str`` with the name of a registered pipeline.\n",
    "#                 * An ``MLPipeline`` instance.\n",
    "#                 * A ``dict`` with an ``MLPipeline`` specification.\n",
    "#         hyperparameters (dict):\n",
    "#             Additional hyperparameters to set to the Pipeline.\n",
    "#     \"\"\"\n",
    "\n",
    "#     PIPELINES_DIR = tuple(\n",
    "#         dirname\n",
    "#         for dirname, _, _ in os.walk(os.path.join(os.path.dirname(__file__), 'pipelines'))\n",
    "#         if os.path.exists(os.path.join(dirname, os.path.basename(dirname) + '.json'))\n",
    "#     )\n",
    "#     PIPELINES = tuple(os.path.basename(pipeline) for pipeline in PIPELINES_DIR)\n",
    "\n",
    "#     DEFAULT_PIPELINE = 'lstm_dynamic_threshold'\n",
    "\n",
    "#     def _get_mlpipeline(self):\n",
    "#         pipeline = self._pipeline\n",
    "#         if isinstance(pipeline, str) and os.path.isfile(pipeline):\n",
    "#             with open(pipeline) as json_file:\n",
    "#                 pipeline = json.load(json_file)\n",
    "\n",
    "#         mlpipeline = MLPipeline(pipeline)\n",
    "#         if self._hyperparameters:\n",
    "#             mlpipeline.set_hyperparameters(self._hyperparameters)\n",
    "\n",
    "#         return mlpipeline\n",
    "\n",
    "#     def __init__(self, pipeline: Union[str, dict, MLPipeline] = None,\n",
    "#                  hyperparameters: dict = None):\n",
    "#         self._pipeline = pipeline or self.DEFAULT_PIPELINE\n",
    "#         self._hyperparameters = hyperparameters\n",
    "#         self._mlpipeline = self._get_mlpipeline()\n",
    "#         self._fitted = False\n",
    "\n",
    "\n",
    "#     def __eq__(self, other):\n",
    "#         return (\n",
    "#             isinstance(other, self.__class__) and\n",
    "#             self._pipeline == other._pipeline and\n",
    "#             self._hyperparameters == other._hyperparameters and\n",
    "#             self._fitted == other._fitted\n",
    "#         )\n",
    "\n",
    "#     def fit(self, data: pd.DataFrame, **kwargs):\n",
    "#         \"\"\"Fit the pipeline to the given data.\n",
    "\n",
    "#         Args:\n",
    "#             data (DataFrame):\n",
    "#                 Input data, passed as a ``pandas.DataFrame`` containing\n",
    "#                 exactly two columns: timestamp and value.\n",
    "#         \"\"\"\n",
    "#         if not self._fitted:\n",
    "#             self._mlpipeline = self._get_mlpipeline()\n",
    "\n",
    "#         self._mlpipeline.fit(data, **kwargs)\n",
    "#         self._fitted = True\n",
    "\n",
    "\n",
    "#     def _get_outputs_spec(self):\n",
    "#         outputs_spec = [\"default\"]\n",
    "#         try:\n",
    "#             visualization_outputs = self._mlpipeline.get_output_names('visualization')\n",
    "#             outputs_spec.append('visualization')\n",
    "#         except ValueError:\n",
    "#             visualization_outputs = []\n",
    "\n",
    "#         return outputs_spec, visualization_outputs\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _build_events_df(events):\n",
    "#         events = pd.DataFrame(list(events), columns=['start', 'end', 'severity'])\n",
    "#         events['start'] = events['start'].astype('int64')\n",
    "#         events['end'] = events['end'].astype('int64')\n",
    "\n",
    "#         return events\n",
    "\n",
    "#     def _detect(self, method, data, visualization=False, **kwargs):\n",
    "#         if visualization:\n",
    "#             outputs_spec, visualization_names = self._get_outputs_spec()\n",
    "#         else:\n",
    "#             outputs_spec = 'default'\n",
    "\n",
    "#         outputs = method(data, output_=outputs_spec, **kwargs)\n",
    "\n",
    "#         if visualization:\n",
    "#             if visualization_names:\n",
    "#                 events = outputs[0]\n",
    "#                 visualization_outputs = outputs[-len(visualization_names):]\n",
    "#                 visualization_dict = dict(zip(visualization_names, visualization_outputs))\n",
    "#             else:\n",
    "#                 events = outputs\n",
    "#                 visualization_dict = {}\n",
    "\n",
    "#             return self._build_events_df(events), visualization_dict\n",
    "\n",
    "#         return self._build_events_df(outputs)\n",
    "\n",
    "#     def detect(self, data: pd.DataFrame, visualization: bool = False) -> pd.DataFrame:\n",
    "#         \"\"\"Detect anomalies in the given data..\n",
    "\n",
    "#         If ``visualization=True``, also return the visualization\n",
    "#         outputs from the MLPipeline object.\n",
    "\n",
    "#         Args:\n",
    "#             data (DataFrame):\n",
    "#                 Input data, passed as a ``pandas.DataFrame`` containing\n",
    "#                 exactly two columns: timestamp and value.\n",
    "#             visualization (bool):\n",
    "#                 If ``True``, also capture the ``visualization`` named\n",
    "#                 output from the ``MLPipeline`` and return it as a second\n",
    "#                 output.\n",
    "\n",
    "#         Returns:\n",
    "#             DataFrame or tuple:\n",
    "#                 If visualization is ``False``, it returns the events\n",
    "#                 DataFrame. If visualization is ``True``, it returns a\n",
    "#                 tuple containing the events DataFrame followed by the\n",
    "#                 visualization outputs dict.\n",
    "#         \"\"\"\n",
    "#         return self._detect(self._mlpipeline.predict, data, visualization)\n",
    "\n",
    "\n",
    "#     def fit_detect(self, data: pd.DataFrame, visualization: bool = False,\n",
    "#                    **kwargs) -> pd.DataFrame:\n",
    "#         \"\"\"Fit the pipeline to the data and then detect anomalies.\n",
    "\n",
    "#         This method is functionally equivalent to calling ``fit(data)``\n",
    "#         and later on ``detect(data)`` but with the difference that\n",
    "#         here the ``MLPipeline`` is called only once, using its ``fit``\n",
    "#         method, and the output is directly captured without having\n",
    "#         to execute the whole pipeline again during the ``predict`` phase.\n",
    "\n",
    "#         If ``visualization=True``, also return the visualization\n",
    "#         outputs from the MLPipeline object.\n",
    "\n",
    "#         Args:\n",
    "#             data (DataFrame):\n",
    "#                 Input data, passed as a ``pandas.DataFrame`` containing\n",
    "#                 exactly two columns: timestamp and value.\n",
    "#             visualization (bool):\n",
    "#                 If ``True``, also capture the ``visualization`` named\n",
    "#                 output from the ``MLPipeline`` and return it as a second\n",
    "#                 output.\n",
    "\n",
    "#         Returns:\n",
    "#             DataFrame or tuple:\n",
    "#                 If visualization is ``False``, it returns the events\n",
    "#                 DataFrame. If visualization is ``True``, it returns a\n",
    "#                 tuple containing the events DataFrame followed by the\n",
    "#                 visualization outputs dict.\n",
    "#         \"\"\"\n",
    "#         if not self._fitted:\n",
    "#             self._mlpipeline = self._get_mlpipeline()\n",
    "\n",
    "#         result = self._detect(self._mlpipeline.fit, data, visualization, **kwargs)\n",
    "#         self._fitted = True\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def save(self, path: str):\n",
    "#         \"\"\"Save this object using pickle.\n",
    "\n",
    "#         Args:\n",
    "#             path (str):\n",
    "#                 Path to the file where the serialization of\n",
    "#                 this object will be stored.\n",
    "#         \"\"\"\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         with open(path, 'wb') as pickle_file:\n",
    "#             pickle.dump(self, pickle_file)\n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def load(cls, path: str):\n",
    "#         \"\"\"Load an Orion instance from a pickle file.\n",
    "\n",
    "#         Args:\n",
    "#             path (str):\n",
    "#                 Path to the file where the instance has been\n",
    "#                 previously serialized.\n",
    "\n",
    "#         Returns:\n",
    "#             Orion\n",
    "\n",
    "#         Raises:\n",
    "#             ValueError:\n",
    "#                 If the serialized object is not an Orion instance.\n",
    "#         \"\"\"\n",
    "#         with open(path, 'rb') as pickle_file:\n",
    "#             orion = pickle.load(pickle_file)\n",
    "#             if not isinstance(orion, cls):\n",
    "#                 raise ValueError('Serialized object is not an Orion instance')\n",
    "\n",
    "#             return orion\n",
    "\n",
    "\n",
    "#     def evaluate(self, data: pd.DataFrame, ground_truth: pd.DataFrame, fit: bool = False,\n",
    "#                  train_data: pd.DataFrame = None, metrics: List[str] = METRICS) -> pd.Series:\n",
    "#         \"\"\"Evaluate the performance against ground truth anomalies.\n",
    "\n",
    "#         Args:\n",
    "#             data (DataFrame):\n",
    "#                 Input data, passed as a ``pandas.DataFrame`` containing\n",
    "#                 exactly two columns: timestamp and value.\n",
    "#             ground_truth (DataFrame):\n",
    "#                 Ground truth anomalies passed as a ``pandas.DataFrame``\n",
    "#                 containing two columns: start and stop.\n",
    "#             fit (bool):\n",
    "#                 Whether to fit the pipeline before evaluating it.\n",
    "#                 Defaults to ``False``.\n",
    "#             train_data (DataFrame):\n",
    "#                 Training data, passed as a ``pandas.DataFrame`` containing\n",
    "#                 exactly two columns: timestamp and value.\n",
    "#                 If not given, the pipeline is fitted on ``data``.\n",
    "#             metrics (list):\n",
    "#                 List of metrics to used passed as a list of strings.\n",
    "#                 If not given, it defaults to all the Orion metrics.\n",
    "\n",
    "#         Returns:\n",
    "#             Series:\n",
    "#                 ``pandas.Series`` containing one element for each\n",
    "#                 metric applied, with the metric name as index.\n",
    "#         \"\"\"\n",
    "#         if not fit:\n",
    "#             method = self._mlpipeline.predict\n",
    "#         else:\n",
    "#             if not self._fitted:\n",
    "#                 mlpipeline = self._get_mlpipeline()\n",
    "\n",
    "#             if train_data is not None:\n",
    "#                 # Fit first and then predict\n",
    "#                 mlpipeline.fit(train_data)\n",
    "#                 method = mlpipeline.predict\n",
    "#             else:\n",
    "#                 # Fit and predict at once\n",
    "#                 method = mlpipeline.fit\n",
    "\n",
    "#         events = self._detect(method, data)\n",
    "\n",
    "#         scores = {\n",
    "#             metric: METRICS[metric](ground_truth, events, data=data)\n",
    "#             for metric in metrics\n",
    "#         }\n",
    "\n",
    "#         return pd.Series(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64907ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_venv",
   "language": "python",
   "name": "orion_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
